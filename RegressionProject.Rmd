---
title: "Regression Model Course Project"
author: "Nazmi Anik"
date: "Saturday, April 02, 2016"
output: html_document
---

## Executive Summary  


## Exploratory Data Analysis  

```{r library, message=FALSE, echo=FALSE}
require(ggplot2) #import ggplot library
```
``` {r load, message=FALSE,echo=TRUE}
data(mtcars) #load mtcars data
str(mtcars) #checkout the variables in mtcars
data.frame(cars) <- subset(mtcars)
```
We can see that the following variables should be converted to factor variables for accuracy of our assessment:  
```{r factor, echo=TRUE}
mtcars2 <- mtcars
mtcars2$cyl <- factor(mtcars$cyl)
mtcars2$vs <- factor(mtcars$vs)
mtcars2$gear <- factor(mtcars$gear)
mtcars2$carb <- factor(mtcars$carb)
mtcars2$am <- factor(mtcars$am,labels=c('Automatic','Manual'))
```
Let's check the correlation matrix for to see what kind of relationship exists between the variables and *mpg*. We will square the result for easy comparison. We are using original *mtcars* data frame with numeric values to be able to use *cor* function.   
```{r correlation, echo=TRUE}
corCars <- cor(mtcars) #form correlation matrix
corCars[,1]^2 #display first row, squared
```
In our preliminary analysis, it looks like *mpg* is highly correlated with *cyl*, *disp*, *hp* and *wt* variables.  
Since we are asked to analyze the relationship between transmission and *mpg*, let's check that relationship with a boxplot:  
```{r box, echo=TRUE, fig.height=4, fig.width=6}
boxplot(mpg ~ am, data = mtcars2, col = (c("green","blue")), main="MPG vs Transmission", ylab = "Miles Per Gallon", xlab = "Transmission Type")
```
Our initial analysis showed us that manual transmission may be more fuel efficient than automatic. Now we need to check the validity of our data and see if there are any dependencies between variables that might be skewing our results.  

## Model Fitting  

Let's try to find the best model to fit. First we will fit all the variables. Then let's use *step* function to find the best fit.  
Source for *step* function usage is [here](http://www.stat.columbia.edu/~martin/W2024/R10.pdf)
```{r fit, echo=TRUE}
fit0 <- lm(mpg ~ am, mtcars2) #our starting model
fit1 <- lm(mpg ~ ., mtcars2) #our full model
fit2 <- step(fit0, scope=list(upper=fit1),direction="both",trace=0) #create the best fit
summary(fit2)
```
Looking at the summary, we see that the best model includes *am*, *wt*, *qsec* as the regressors. R^2 value shows us that our
model explains 84% of the variability. Looking at the p-values, we can see that all of these regressors are significant with
an alpha = 0.05.  

## Diagnostics  

Now let's examine our residuals to see if there are any issues with our model.  
```{r resid, echo=TRUE}
par(mfrow=c(2, 2))
plot(fit2)
```
Properties that we find out based on the plots:  
Independence: Residuals vs Fitted values plot shows no specific pattern.  
Normality: Normal Q-Q plot shows us that residuals are normally distributed as they are close to the line.  
Constant Covariance: Scale-Location plot affirms the constant variance, since the points are randomly distributed.  
No Outliers: Residuals vs. Leverage shows all values within the 0.5 band, therefore no influential outliers are present.  

Let's check some of the outliers to confirm that the outliers have no significant leverage:  
```{r hatvalue, echo=TRUE}
summary(round(hatvalues(fit2),3))
```
The hat values are not too far from one another, therefore, we can confirm that there are no influential outliers.  

## Results  



## Appendix  
